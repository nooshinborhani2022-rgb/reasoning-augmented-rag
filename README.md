# Reasoning-Augmented RAG Systems

This repository documents my Master’s thesis research on the role of reasoning in
Retrieval-Augmented Generation (RAG) systems, with a specific focus on interpretability,
faithfulness, and epistemic reliability.

Rather than treating large language models as purely performance-driven tools,
this project approaches them as epistemic systems whose reasoning strategies shape
how meaning, justification, and authority are produced.

---

## Research Context

This project is situated at the intersection of natural language processing and digital
humanities, approaching RAG systems as epistemic infrastructures rather than neutral
information tools. It investigates how reasoning-oriented prompting shapes interpretation,
response structure, and qualitative output characteristics in AI-driven systems.

---

## Thesis

**How Can Reasoning Prompts Improve the Quality of Responses in RAG-based AI-driven Systems?**

The thesis investigates how different reasoning-augmented prompting strategies influence
the quality of responses generated by RAG systems beyond factual accuracy.

The study is conducted on an AI-driven tender assistant (AI4Tender) and evaluates model
behavior across multiple qualitative dimensions.

The thesis document is available in the `thesis/` directory.

---

## Reasoning Strategies

The project evaluates multiple reasoning strategies:

- **Baseline Prompt** – iterative refinement of the original chatbot prompt
- **Universal Self-Consistency (USC)**
- **Analogical Reasoning**
- **Faithful Reasoning**
- **Thread of Thought (ThoT)**

Prompt templates used in the experiments are available in the `prompts/` directory.

---

## Evaluation Framework

Instead of relying solely on standard accuracy metrics, responses are evaluated through
a qualitative, human-centered framework that considers:

- Informativeness
- Justification
- Focus
- Coherence

Model performance is analyzed across five question types, including direct retrieval,
comparative, exploratory, abstract reasoning, and multi-intent queries, capturing
different cognitive and interpretative demands.

A detailed description of the evaluation criteria is provided in
`evaluation/metrics_description.md`.

---

## Key Findings

The analysis shows that reasoning-augmented prompting strategies significantly influence
the qualitative characteristics of RAG system outputs. In particular, strategies that
explicitly structure reasoning processes tend to improve response coherence,
justificatory depth, and focus, especially for exploratory and abstract queries.

Conversely, prompts lacking explicit reasoning guidance often produce surface-level
answers that prioritize retrieval over interpretation. These findings suggest that
reasoning prompts play a critical role in shaping not only what models answer, but how
they construct and justify their responses.

---

##  Personal Contributions

My personal contributions include:

- Design of reasoning-augmented prompting strategies
- Integration of reasoning prompts into a modular RAG pipeline
- Definition of a qualitative evaluation framework
- Systematic analysis of model reasoning behavior beyond surface-level outputs
- Showed how reasoning-oriented prompting strategies improve the qualitative structure
  and epistemic clarity of system responses

---

## Notes on Reproducibility

Due to confidentiality constraints related to the application domain, parts of the
implementation are not publicly released.

However, the experimental design, prompting strategies, and evaluation methodology
are fully documented. Additional materials can be made available upon request.

---

## Author

**Nooshin Borhani Rayeni**  
Master’s student in Language Technologies and Digital Humanities  
Research interests: reasoning in AI systems, interpretability, and the study of AI models as epistemic and cultural systems.


