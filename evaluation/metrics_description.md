# Evaluation Criteria

The evaluation framework used in this project is designed to assess the qualitative
behavior of responses generated by Retrieval-Augmented Generation (RAG) systems,
with a focus on reasoning, interpretability, and response structure rather than
accuracy alone.

Responses are evaluated along the following dimensions:

## Informativeness
Measures the extent to which the response provides relevant and sufficient information
to address the user query, beyond surface-level or generic answers.

## Justification
Evaluates whether the response offers clear reasoning, explanations, or supporting
arguments for its claims, rather than presenting conclusions without rationale.

## Focus
Assesses how well the response maintains alignment with the original question and avoids
irrelevant digressions or unnecessary information.

## Coherence
Examines the logical organization and internal consistency of the response, including
clarity of progression and absence of contradictions.

## Question Types
Responses are analyzed across five distinct question categories to evaluate how
reasoning-oriented prompting strategies perform under different cognitive and
interpretative demands. The question types are defined as follows:

- **Direct Retrieval Questions**, which require identifying and presenting relevant
  information directly from retrieved documents (e.g., available funding calls or
  open tenders).

- **Comparative Queries**, which involve comparing multiple domains, programs, or
  funding opportunities to highlight differences, similarities, or relative advantages.

- **Exploratory Prompts**, which represent open-ended queries aimed at surveying
  possible options, opportunities, or directions within a given thematic space.

- **Abstract Reasoning Questions**, which require higher-level reasoning to infer
  motivations, policy objectives, or conceptual relationships underlying funding calls.

- **Multi-Intent Queries**, which combine multiple goals or constraints within a
  single query and require the model to integrate and prioritize information across
  different criteria.

This categorization enables a fine-grained qualitative analysis of model behavior and
supports a systematic comparison of reasoning strategies across varying levels of
complexity.
